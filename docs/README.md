#  Introduction

Semi-sklearn是一个有效易用的半监督学习工具包。目前该工具包包含30种半监督学习算法，其中基于传统机器学习模型的算法13种，基于深度神经网络模型的算法17种，可用于处理结构化数据、图像数据、文本数据、图结构数据4种数据类型，可用于分类、回归、聚类3种任务，包含数据管理、数据变换、算法应用、模型评估等多个模块，便于实现端到端的半监督学习过程，兼容目前主流的机器学习工具包scikit-learn和深度学习工具包pytorch，具备完善的功能，标准的接口和详尽的文档。

Semi-sklearn is an useful and efficient toolbox for semi-supervised learning. At present, Semi-sklearn contains 30 semi-supervised learning algorithms, including 13 algorithms based on classical machine learning models and 17 algorithms based on deep neural network models. Semi-sklearn can be used to process four types of data: structured data, image data, text data and graph data, and can be used for three types of task: classification, regression and clustering. Semi-sklearn includes multiple modules such as data management, data transformation, algorithm application, model evaluation and so on, which facilitates the completion of the end-to-end semi-supervised learning process, and is compatible with the current popular machine learning toolkit scikit-learn and deep learning toolkit pytorch.

## Design Idea


Semi-sklearn的整体设计思想如图所示。Semi-sklearn参考了sklearn工具包的底层实现，所有算法都使用了与sklearn相似的接口。 在sklearn中的学习器都继承了Estimator这一父类，Estimator表示一个估计器，利用现有数据建立模型对未来的数据做出预测，对估计器存在fit()和transform()两个方法，其中fit()方法是一个适配过程，即利用现有数据建立模型，对应了机器学习中的训练过程，transform()方法是一个转换过程，即利用fit()过后的模型对新数据进行预测。

The overall design idea of Semi-sklearn is shown in the figure. Semi-sklearn refers to the underlying implementation of sklearn. All Algorithms in Semi-sklearn have interfaces similar to that Algorithms in sklearn.The learners in sklearn all inherit the parent class Estimator. Estimator uses known data to build a model to make predictions on unknown data. There are usually two usual methods in Estimator: fit() and transform(). fit() method is an adaptation process using existing data to build a model, which corresponds to the training process in machine learning. transform() method is a transformation process using the fitted model to predict results of new instances, which corresponds to the predicting process in machine learning.
<div align=center>
<img width="500px"  src="./Imgs/Base.png" >
</div>

Semi-sklearn中的预测器通过继承半监督预测器类SemiEstimator间接继承了sklearn中的Estimator。由于sklearn中fit()方法使用的数据往往包含样本和标注两项，在半监督学习中，模型的训练过程中同时使用有标注数据、标注和无标注数据，因此Estimator的fit()方法不方便直接用于半监督学习算法。虽然sklearn中也实现了自训练方法和基于图的方法两类半监督学习算法，它们也继承了Estimator类，但是为了使用fit()方法的接口，sklearn将有标注样本与无标注数据样本结合在一起作为fit()的样本输入，将标注输入中无标注数据对应的标注记为-1，这种处理方式虽然可以适应Estimator的接口，但是也存在局限性，尤其使在一些二分类场景下往往用-1表示有标注数据的负例标注，与无标注数据会发生冲突，因此针对半监督学习在Estimator的基础上重新建立新类SemiEstimator具有必要性，SemiEstimator的fit()方法包含有标注数据、标注和无标注数据三部分输入，更好地契合了半监督学习的应用场景，避免了要求用户自己对数据进行组合处理，也避免了无标注数据与二分类负类的冲突，相较Estimator使用起来更加方便。

Learners in Semi-sklearn indirectly inherits Estimator in sklearn by inheriting the semi-supervised predictor class SemiEstimator. Data used for fit() method in sklearn usually includes samples and labels.However, in semi-supervised learning, labeled samples, labels and unlabeled samples are used in the training process of the model, so Estimator's fit() method is inconvenient directly used in semi-supervised learning algorithms. Although sklearn also implements two types of semi-supervised learning algorithms, self-training methods and graph-based methods, which also inherit the Estimator class, in order to use the interface of the fit() method, sklearn combines labeled samples and unlabeled data samples as samples input of fit() method, mark the labels corresponding to the unlabeled samples as -1 in labels input of fit() method. Although this processing method can be adapted to the Estimator interface, it also has limitations, especially in some binary classification scenario using -1 to indicate negative labels of labeled samples, which will conflict with unlabeled samples. Therefore, it is necessary to re-establish a new class SemiEstimator on the basis of Estimator for semi-supervised learning. There are three parts in the input of SemiEstimator's fit() method: labeled samples, labels and unlabeled samples, which better adapts the application scenario of semi-supervised learning. It doesn't require users combining data by themselves, and avoids the conflict between marks of unlabeled samples and labels of negative samples in binary classification. Compared with Estimator it's more convenient.

半监督学习一般分为归纳式学习和直推式学习，区别在于是否直接使用待预测数据作为训练过程中的无标注数据。Semi-sklearn中使用两个类InductiveEstimator和Transductive分别对应了归纳式学习和直推式学习两类半监督学习方法，均继承了SemiEstimator类。

Semi-supervised learning is generally divided into inductive learning and transductive learning. The difference is whether the samples to be predicted is directly used as the unlabeled samples in the training process. Semi-sklearn uses two classes InductiveEstimator and TransductiveEstimator, which correspond to two types of semi-supervised learning methods: inductive learning and transductive learning respectively. InductiveEstimator and TransductiveEstimator both inherit SemiEstimator.

<div align=center>
<img width="500px"  src="./Imgs/LearningPattern.png" > 
</div>

在sklearn中，为了使估计器针对不同的任务可以具备相应的功能，sklearn针对估计器的不同使用场景开发了与场景对应的组件（Mixin），sklearn中的估计器往往会同时继承Estimator和相应组件，从而使估计器同时拥有基本的适配和预测功能，还能拥有不同组件对应的处理不同任务场景的功能。其中关键组件包括用于分类任务的ClassifierMixin、用于回归任务的RegressorMixin、用于聚类任务的ClusterMixin和用于数据转换的TransformerMixin，在Semi-sklearn中同样使用了这些组件。

In order to enable estimators to have corresponding functions for different tasks, sklearn has developed components (Mixin) corresponding to the scene for different usage scenarios of estimators. Estimators in sklearn often inherit both Estimator and corresponding components, so that they have the most basic fitting and prediction capabilities and also have the function of processing tasks corresponding to specific components. The key components include ClassifierMixin for classification tasks, RegressorMixin for regression tasks, ClusterMixin for clustering tasks, and TransformerMixin for data transformation, which are also used in Semi-sklearn.

另外，不同于经典机器学习中常用的sklearn框架，深度学习在经常使用pytorch框架，pytorch各组件间存在较大的依赖关系（如图3-2所示），耦合度高，例如数据集（Dataset）与数据加载器（Dataloader）的耦合、优化器（Optimizer）和调度器（Scheduler）的耦合、采样器（Sampler）与批采样器（BatchSampler）的耦合等，没有像sklearn一样的简单的逻辑和接口，对用户自身要求较高，较不方便，为在同一工具包在同时包含经典机器学习方法和深度学习方法造成了较大困难，为了解决经典机器学习方法和深度学习方法难以融合于相同框架的问题，Semi-sklearn用DeepModelMixin这一组件使基于pytorch开发的深度半监督模型拥有了与经典机器学习方法相同接口和使用方式，Semi-sklearn中的深度半监督学习算法都继承了这一组件。DeepModelMixin对pytorch各模块进行了解耦，便于用户独立更换深度学习中数据加载器、网络结构、优化器等模块，而不需要考虑更换对其他模块造成的影响，DeepModelMixin会自动处理这些影响，使用户可以像调用经典的半监督学习算法一样便捷地调用深度半监督学习算法。

In addition, different from sklearn framework commonly used in classical machine learning, deep learning often uses pytorch framework. There are lots of dependencies between the components of pytorch such as Dataset coupling with Dataloader, Optimizer coupling with Scheduler, Sampler coupling with BatchSampler, etc. In pytorch, there is no simple logic and interfaces like sklearn which causes great difficulty in integrating both classical machine learning methods and deep learning methods into a same toolkit. In order to solve this problem Semi-sklearn uses DeepModelMixin component to enable deep semi-supervised models developed based on pytorch to have the same interface and usage as classical machine learning methods. Deep semi-supervised learning algorithms in Semi-sklearn all inherit this component. DeepModelMixin decouples each module of pytorch, which is convenient for users to independently replace data loader, network structure, optimizer and other modules in deep learning without considering the impact of replacement on other modules. Deep semi-supervised learning algorithms can be called as easily as classical semi-supervised learning algorithms in Semi-sklearn.

<div align=center>
<img width="600px"  src="./Imgs/PytorchCoupling.png" > 
</div>

## Data Management

Semi-sklearn拥有强大的数据管理和数据处理功能。在Semi-sklearn中，一个半监督数据集整体可以用一个SemiDataset类进行管理，SemiDataset类可以同时管理TrainDataset、ValidDataset、TestDataset三个子数据集，分别对应了机器学习任务中的训练数据集、验证数据集和测试数据集，在最底层数据集分为LabeledDataset和UnlabeledDataset两类，分别对应了半监督学习中的有标注数据与无标注数据，训练集往往同时包含有标注数据和无标注数据，因此TrainDataset同时管理LabeledDataset和UnlabeledDataset两个数据集。

Semi-sklearn针对LabeledDataset和UnlabeledDataset分别设计了LabeledDataloader和UnlabeledDataloader两种数据加载器，而用一个TrainDataloader类同时管理两种加载器用于半监督学习的训练过程，除同时包含两个加载器外，还起到调节两个加载器之间关系的作用，如调节每一批数据中有标注数据与无标注数据的比例。

Semi-sklearn可以处理结构化数据、图像数据、文本数据、图数据四种现实应用中常见的数据类型，分别使用了四个与数据类型对应的组件StructuredDataMixin、VisionMixin、TextMixin、GraphMixin进行处理，对于一个数据集，可以继承与其数据类型对应的组件获得组件中的数据处理功能。

Semi-sklearn has powerful data management and data processing functions. In Semi-sklearn, a semi-supervised dataset can be managed by SemiDataset class. The SemiDataset class can manage three sub-datasets: TrainDataset, ValidDataset, and TestDataset corresponding to training dataset, validation dataset and test dataset in machine learning tasks respectively. The most basic classes for data management are LabeledDataset and UnlabeledDataset, which correspond to labeled data and unlabeled data in semi-supervised learning respectively. Training datasets often contains both labeled data and unlabeled data. Therefore, TrainDataset simultaneously manage a LabeledDataset and an UnlabeledDataset. 

Semi-sklearn designs two most basic data loaders: LabeledDataloader and UnlabeledDataloader for LabeledDataset and UnlabeledDataset respectively.Semi-sklarn uses TrainDataloader class to manage the two loaders in the training process of semi-supervised learning and adjust the relationship between the two loaders, such as adjusting the ratio of labeled data and unlabeled data in each batch. 

Semi-sklearn can process structured data, image data, text data, and graph data, which are four common data types in practical applications. Semi-sklearn uses four components corresponding to data types: StructuredDataMixin, VisionMixin, TextMixin, and GraphMixin. Every Dataset can inherit the component corresponding to its data type to obtain the data processing function in the component.

<div align=center>
<img width="600px"  src="./Imgs/Dataset.png" > 
</div>

## Data Transformation

使用机器学习算法利用数据学习模型和用模型对数据进行预测之前通常需要对数据进行预处理或数据增广，尤其是在半监督学习领域，部分算法本身就包含对数据进行不同程度的增广和加噪声的需求，Semi-sklearn的数据变换模块针对不同类型的数据提供了多样的数据预处理和数据增广方法，如对于结构化数据的归一化、标准化、最小最大化等，对于视觉数据的旋转、裁剪、翻转等，对于文本数据的分词、词嵌入、调整长度等，对于图数据的结点特征标准化、k近邻图构建、图扩散等。Semi-sklearn中所有数据变换方法都继承了sklearn中的TransformerMixin类，并且sklearn或pytorch都可以使用。对于依次进行的多次数据变换，sklearn的Pipeline机制和pytorch的Compose机制都可以使用。

Before using data to learn models and using models to predict labels of new data, it is usually necessary to preprocess or augment data, especially in the field of semi-supervised learning. To meet the needs of adding noise, the data transformation module of Semi-sklearn provides various data preprocessing and data augmentation methods for different types of data, such as normalization, standardization, MinMaxScale for structured data, Rotation, cropping, flipping for visual data, word segmentation, word embedding, length adjustment for text data, node feature standardization, k-nearest neighbor graph construction, graph diffusion for graph data, etc. All data transformation methods inherit TransformerMixin class from sklearn. Transformation method can be called using the interface of either sklearn or pytorch. For multi transformations in turn, both Pipeline mechanism in sklearn and Compose   mechanism in pytorch can be used. 

## Algorithm Usage
目前Semi-sklearn包含30种半监督学习算法，其中基于传统机器学习模型的算法13种（如图3-3所示）：半监督支持向量机类方法TSVM、LapSVM，基于图的方法Label Propagation、Label Spreading，生成式方法SSGMM，封装方法Self-Training、Co-Training、Tri-Training，集成方法SemiBoost、Assemble，半监督回归方法CoReg，半监督聚类方法Constrained K Means、Constrained Seed K Means；基于深度神经网络模型的算法17种（如图3-4所示）：一致性正则方法Ladder Network、Pi Model、Temporal Ensembling、Mean Teacher、VAT、UDA，基于伪标注的方法Pseudo Label、S4L，混合方法ICT、MixMatch、ReMixMatch、FixMatch、FlexMatch，生成式方法ImprovedGAN、SSVAE，图神经网络方法SDNE、GCN。

At present, Semi-sklearn contains 30 semi-supervised learning algorithms. There are 13 algorithms based on classical machine learning models, including generative method: SSGMM; semi-supervised support vector machine methods: TSVM, LapSVM; graph-based methods: Label Propagation, Label Spreading;  wrappers methods: Self-Training, Co-Training, Tri-Training; ensemble methods: Assemble, SemiBoost; semi-supervised regression method: CoReg; semi-supervised clustering method: Constrained K Means, Constrained Seed K Means. There are 17 algorithms based on deep neural network models, including Consistency regularization methods: Ladder Network, Pi Model, Temporal Ensembling, Mean Teacher, VAT, UDA; pseudo-label-based methods: Pseudo Label, S4L; hybird methods: ICT , MixMatch, ReMixMatch, FixMatch, FlexMatch; deep generative methods: ImprovedGAN, SSVAE; deep graph based methods: SDNE, GCN.

<div align=center>
<img width="1000px"  src="./Imgs/ClassicalSSL.png" >
</div>

<div align=center> 
<img width="1000px"  src="./Imgs/DeepSSL.png" > 
</div>

## Model Evaluation

Semi-sklearn提供了针对不同任务的不同评估指标，如针对分类任务的准确率、精度、召回率等，针对回归任务的均方误差、均方对数误差、平均绝对误差等，针对聚类任务的Davies Bouldin Index、Fowlkes and Mallows Index、Rand Index等。在Semi-sklearn中，评估方法可以在得到预测结果后调用，也可以用python字典的形式作为参数直接传入模型。

Semi-sklearn provides different evaluation indicators for different tasks, such as accuracy, precision, recall for classification tasks, mean squared error, mean squared logarithmic error, mean absolute error for regression tasks and Davies Bouldin Index, Fowlkes and Mallows Index, Rand Index for clustering tasks etc. In Semi-sklearn, the evaluation method can be called after getting the prediction results directly passed to the model in the form of a python dictionary as a parameter.

# Quick Start

## Load Data

## Transform Data

## Train a Classical SSL Model

## Train a Deep SSL Model

## Use Pipeline Mechanism

## Search Params

## Train Distributedly

## Save and Load Model

# User Guide

## Algorithm

### Classical Semi-supervised Learning

#### Generative Model

生成式半监督学习方法基于生成式模型，其假设数据由一潜在的分布生成而来，生成式方法建立了样本与生成式模型参数之间的关系，而半监督生成式方法将无标注数据的标注视为模型的隐变量数据，可以采用期望-最大化（EM）算法进行极大似然估计求解。

Generative semi-supervised learning methods are based on generative models, which assume that data is generated from a potential distribution. Generative methods establish the relationship between samples and generative model parameters. In semi-supervised generative methods, labels are regarded as the latent variables of the model, The expectation-maximization (EM) algorithm can be used for maximum likelihood estimation.
##### SSGMM

Shahshahani等提出了SSGMM模型。SSGMM即半监督高斯混合模型，假设数据由一个高斯混合模型生成，即样本的边缘分布可以表示为若干个高斯分布混合在一起的结果，且通过混合参数为每个高斯分布赋予一个权重。SSGMM将每一个类别的样本分布对应一个高斯混合成分。对于有标注数据，其类别对应的的高斯混合成分已知，对于无标注数据，其对应的高斯混合成分用一个概率分布表示，并可以将其分类为概率最高的高斯混合成分对应的类别。SSGMM假设样本服从独立同分布，其似然函数为所有有标注数据的样本与标注联合分布与所有无标注数据样本的边缘概率分布的乘积，通过最大似然估计使似然函数最大化，得到使当前有标注数据与无标注数据共同出现概率最大的生成式模型参数，包括高斯混合模型各部分的方差、均值以及权重。由于该方法存在无标注数据的标注这一无法观测的隐变量，因此无法直接求解最大似然参数，因此SSGMM采用了EM算法解决该问题。EM算法分为两步，其中E步根据当前参数与可观测数据得到未观测数据的条件分布或期望，在SSGMM模型中，这一步利用贝叶斯公式跟据已观测样本和当前模型的参数求解了无标注数据属于每一混合成分的概率，即无标注数据类别的条件分布；M步根据当前已观测变量的值与隐变量的期望或概率分布对模型参数进行最大似然估计，即原先由于隐变量未知无法进行最大似然估计，E步之后得到了隐变量的期望或条件概率分布，最大似然估计就变得可行了，在SSGMM模型中，这一步利用已观测到的有标注样本与标注、无标注样本与E步得到的类别条件分布更新了高斯混合模型的参数。E步与M步以一种迭代的形式交替进行，直至收敛，即可实现同时利用有标注数据与无标注数据训练一个高斯混合模型，并通过贝叶斯公式即可得到基于此高斯混合模型的分类器。

SSGMM model was proposed by Shahshahani et al. SSGMM is a semi-supervised Gaussian mixture model. It is assumed that data is generated by a Gaussian mixture model, that is, the marginal distribution of samples can be expressed as the result of mixing several Gaussian distributions together, and each Gaussian distribution is given a weight. SSGMM maps the feature distribution of each class to a Gaussian mixture component. For each labeled instance, the Gaussian mixture component corresponding to its class is known. For each unlabeled instance, the Gaussian mixture component is represented by a probability distribution, and it can be classified into the class corresponding to the Gaussian mixture component with the highest probability. SSGMM assumes that the samples obey the assumption of independent and identical distribution, and its likelihood function is the product of joint probabilities of all labeled examples and marginal probabilities of all unlabeled data samples. The maximum likelihood estimation is used to maximize the likelihood function to get the parameters of the generative model including the variance, mean, and weight of each part of the Gaussian mixture model with the highest probability of co-occurrence of labeled data and unlabeled data. Since this method has unobservable hidden variables corresponding labels of unlabeled samples, it cannot directly get maximum likelihood parameters, so SSGMM adopts the EM algorithm to solve this problem. The EM algorithm is divided into two steps. In E step, conditional distribution or expectation of the unobserved data are obtained according to the current parameters and the observable data. In the SSGMM model, this step uses the Bayesian formula to get the conditional distribution of the labels of the unlabeled samples according to the observed samples and the parameters of the current model. The M-step makes a maximum likelihood estimation of the model parameters according to the value of the currently observed variable and the expectation or probability distribution of the latent variable, that is, the original The hidden variables are unknown and the maximum likelihood estimation cannot be performed. After the E step, the expectation or conditional probability distribution of the hidden variables is obtained, and the maximum likelihood estimation becomes feasible. In the SSGMM model, this step uses the observed labeled samples and labels, unlabeled samples and class conditional distributions obtained in step E update the parameters of the Gaussian mixture model. Step E and step M are carried out alternately in an iterative form until convergence, which can realize the simultaneous use of labeled data and unlabeled data to train a Gaussian mixture model and the classifier based on this Gaussian mixture model can be obtained through the Bayesian formula.

#### <font color=purple size=32>Semi-supervised Support Vactor Machine</font>

支持向量机是机器学习领域最具代表性的算法之一。该类算法将二分类问题视为在样本空间中寻找合适的划分超平面。在线性可分的情况下，在所有能够完成正确分类的超平面中，最优的划分超平面应尽量位于不同类样本间的中间位置，可以提高模型对于未知样本进行预测的稳健性，即将各类样本中距离超平面最近的样本称为支持向量，不同类的支持向量距离超平面的距离相等，支持向量机算法的目的在于寻找距离其对应的支持向量最近的超平面。然而，在现实任务中，往往不存在一个可以将所有训练样本正确划分的超平面，即使存在，也难免会存在过拟合现象，因此一类支持向量机方法引入了软间隔（Soft Margin）机制，即允许超平面不必将所有样本正确分类，而是在优化目标中增加了对分类错误样本的惩罚。

Support vector machine is one of the most representative algorithms in the field of machine learning. This class of algorithms treats the binary classification problem as finding a suitable partitioning hyperplane in the feature space. In the case of linear separability, among all the hyperplanes that can complete the correct classification, the optimal dividing hyperplane should be located in the middle of the samples from different classes, which can improve the robustness of the model for predicting unknown samples. In each class, the sample closest to the hyperplane is called the support vector. Support vectors of different classes are equidistant from the hyperplane. The purpose of the support vector machine algorithm is to find the hyperplane closest to its corresponding support vectors. However, in real tasks, there are often no hyperplanes that can correctly divide all training samples. Even if there are, it is most likely due to overfitting. Therefore, a class of support vector machine methods introduces Soft Margin mechanism , which allows the hyperplane to not necessarily classify all samples correctly, but adds a penalty for misclassifying samples in the optimization objective.

半监督支持向量机是支持向量机算法在半监督学习理论的推广。半监督支持向量机引入了低密度假设，即学习得到的超平面除了需要基于有标注样本使分类尽可能分开，也要尽可能穿过所有样本分布的低密度区域，从而合理利用无标注样本。Semi-sklearn包含了两个半监督支持向量机方法：TSVM和LapSVM。

Semi-supervised support vector machine is a generalization of the support vector machine algorithm in the field of semi-supervised learning. The semi-supervised support vector machine introduces a low-density assumption, that is, the learned hyperplane not only needs to separate the classifications as much as possible based on the labeled samples, but also needs to pass through the low-density regions of the distribution of all samples as much as possible. It make reasonable use of the unlabeled samples. Semi-sklearn includes two semi-supervised SVM methods: TSVM and LapSVM.

##### <font color=blue size=16>TSVM</font>

Joachims等[5]提出的TSVM是最基础的半监督支持向量机方法（如图2-1所示），是一种直推式方法。TSVM需要为每个无标注样本确定其标注，并寻找一个在有标注样本和无标注样本上间隔最大化的划分超平面。由于为无标注样本分配的标注并不一定是其真实有标注，在训练的初始阶段不能将无标注样本与有标注样本一视同仁，TSVM利用参数C_l和C_u分别代表对于有标注样本和无标注样本的惩罚量级，反应了对有标注样本和无标注样本重视程度。由于所有无标注样本的标注可能情况数量随无标注样本数量的增加呈指数级别上升，无法通过穷举的方式确定无标注样本的标注寻找全局最优解，TSVM使用了一种迭代的搜索方法为优化目标寻找近似解：首先基于有标注样本训练一个SVM，并用这个SVM对无标注样本进行预测；之后初始化C_l\llC_u，并开始迭代，在迭代过程中利用所有样本求解新的超平面，并不断寻找一对可能都发生错误预测的无标注异类样本交换标注并重新训练，直到不再能找到符合条件的异类样本，通过加倍C_u的值增加对无标注样本的重视程度，开始新一轮的迭代，直到C_u与C_l相等；最后将得到的模型对无标注样本的预测结果作为无标注样本的标注，完成直推过程。

TSVM  model was proposed by Joachims et al. TSVM is the most basic transductive semi-supervised support vector machine method. TSVM needs to infer labels of unlabeled samples and find a dividing hyperplane that maximizes the distance from support vectors. Since the labels assigned to the unlabeled samples are not necessarily real labels, the unlabeled samples and the labeled samples cannot be treated the same in the initial stage of training. $C_l$ and $C_u$ are magnitudes of the penalty reflecting the importance attached to the labeled samples and the unlabeled samples. Since the number of possible situations where labels of all unlabeled samples may appear increases exponentially with the increase of the number of unlabeled samples, it is impossible to determine the labels of unlabeled samples in an exhaustive way to find the global optimal solution. The optimization goal of TSVM is to find an approximate solution using an iterative search method. Firstly an SVM is trained based on the labeled samples, and the SVM is used to predict the labels of unlabeled samples. Then $C_l\ll C_u $ is initialized, and the iteration is started. In the iterative process, all the samples are used to solve the new hyperplane. TSVM algorithm continuously finds pairs of unlabeled heterogeneous samples that may be mispredicted and exchange labels and retrain the SVM model until no more qualified pairs can be found. The importance of unlabeled samples is increased by doubling the value of $C_u$ in each iteration. Iteration continues until $C_u$ is equal to $ C_l$. Finally, the prediction results of the unlabeled samples obtained by the final SVM model and the transductive process is completed.

##### <font color=blue size=56>LapSVM</font>

Belkin等[6]基于<font color=red>**流形假设**</font>提出了LapSVM。经典的SVM算法追求使支持向量间隔最大化，这只考虑了样本在特征空间的分布情况，然而在实际应用中，高维空间中的样本往往都分布在低维的黎曼流形上，仅依赖于样本在特征空间的间隔进行划分的支持向量机容易忽视样本分布的本质特征。LapSVM在SVM的优化目标的基础上增加了一项流形正则化项，对样本的本质分布进行了引导。LapSVM在所有样本的基础上构建了图模型，通过样本的特征间的相似性得到图模型的权重矩阵，并计算其Laplace矩阵，通过Laplace正则项引导模型对于图中临近样本的预测结果尽可能一致。不同于TSVM，LapSVM仅对有标注样本的错误分类进行惩罚，但是在构建图模型时同时使用了所有样本，从而利用样本在流形上的分布使无标注样本参与了学习的过程。

LapSVM was proposed by Belkin et al. LapSVM is based on the manifold assumption. The classical SVM algorithm seeks to maximize the margin between the dividing hyperplane and support vectors, which only considers the distribution of samples in the feature space. However, in practical applications, samples in high-dimensional space are often distributed on low-dimensional Riemannian manifolds. Classical SVM based on original feature space tends to ignore the essential characteristics of samples. LapSVM adds a manifold regularization term to the optimization objective of SVM for learning the essential distribution of samples. LapSVM builds a graph model using all samples, obtains the weight matrix of the graph model through the similarity between the features of the samples and calculates its Laplace matrix. The Laplace regularization term guides the predicted results of the adjacent samples in the graph to be as consistent as possible. Unlike TSVM, LapSVM only penalizes the misclassification of labeled samples, but uses all samples when building a graph model, so that unlabeled samples participate in the learning process by the distribution of samples on the manifold.

#### Graph Based Method

基于图的半监督学习方法将数据集表示为一个图结构模型，以样本为节点，以样本间的关系为边，在半监督学习中，存在有标注数据与无标注数据，因此图中的结点有一部分存在标注，而另一部分没有标注，因此基于图的直推式半监督学习可以被视为标注在图中传播的过程。

The graph-based semi-supervised learning method represents the data set as a graph model with samples as nodes and relationships among samples as edges. In semi-supervised learning, there are labeled data and unlabeled data, so some points have labels, while others haven't. Graph-based transductive semi-supervised learning can be regarded as the process of label propagation or spreading in the graph.


##### Label Propagation

Zhu等[7]提出了Label Propagation算法。该算法以数据集中的样本为结点，样本间的关系为边进行全连接或k近邻构图，边权往往采用样本在原始数据空间或核空间（其中高斯核最为常用）的相似度进行表示。Label Propagation算法的目的在于将有标注数据的标注通过图结构向无标注数据传播，完成对无标注数据的预测，实现直推式半监督学习。Label Propagation的优化目标为模型预测结果在图结构中的Laplacian一致性正则项，即以边权为权重，将相邻节点间模型预测结果差异的加权均方误差作为优化目标。由于有标注数据的标注是固定的，因此Label Propogation仅需将Laplacian一致性正则项作为优化目标，求解无标注数据的标注使优化目标取最小值，即模型对于图上临近点的预测应该尽可能一致，使优化目标对无标注数据的标注求偏导数，使其偏导数为0，就可以得到其最优解，经证明这个通过直接计算得到的闭式最优解与不断迭代进行无限次标注传播最终收敛到的结果是一致的。通过直接的推导即可求得精确解，不需要模拟标注传递的过程，不需要为了收敛进行多次迭代，这也是Label Propagation对于其他图半监督学习方法的优势所在。

Label Propagation was proposed by Zhu et al. Label Propagation uses samples as nodes, and the relationship between the samples as edges. The graph can be constructed fully connected or based on k-nearest neighbors. The purpose of Label Propagation algorithm is to propagate the labels from labeled data to unlabeled data through the graph. The optimization goal of Label Propagation is the Laplacian consistency, that is the weighted mean square error of the difference of labels between pairs of adjacent nodes. Since the labels of labeled samples are fixed, Label Propogation only needs to solve the labels of unlabeled data to minimize the optimization goal. So the model's predictions for adjacent points on the graph should be as consistent as possible. Label Propagation makes the partial derivative of the optimization objective to the labels of unlabeled samples to be 0 and the optimal solution can be obtained. It has been proved that this closed-form optimal solution obtained by direct calculation is consistent with the final convergence result of infinite and continuous iterations. An accurate solution can be obtained through direct derivation, without simulating the process of label propagation and performing multiple iterations for convergence, which is the advantage of Label Propagation over other graph based semi-supervised learning methods.

##### Label Spreading

Zhou等[9]提出了Label Spreading算法。不同于Label Propagation算法在传播过程中固定了有标注数据的标注，使其在整个传播过程中都保持不变，这保护了真实有标注数据对模型的影响，但是对于存在数据噪声的情况，Label Prapogation会存在一定的局限性，且Label Propagation算法中标注全部指向无标注数据，这可能会堵塞一些需要通过有标注数据结点进行传播的路径，使信息在图中的传播收到了一定的限制。而Label Spreading算法使标注可以在对所有临近结点进行广播，对于传播后结果与真实标注不符的有标注数据，Label Spreading会对其进行惩罚，而不是完全禁止。Label Spreading的优化目标有两项，第一项与Label Propagation的优化目标相同，但不存在模型对有标注数据预测结果必须等于其真实标注这一限制，第二项为对有标注数据预测损失的惩罚，需要设置一个惩罚参数作为其权重。由于优化目标不同，Label Propagation存在闭式解，而Label Spreading则需要通过迭代的方式求解，需要设置一个迭代折中参数，即每轮迭代都需要将本轮迭代得到的传播结果与样本的初始标注利用折中参数进行权衡作为当前预测结果。

 Label Spreading was proposed by Zhou et al. Different from Label Propagation algorithm in which the labels of the labeled samples are fixed during the spreading process to protect the influence of the real labels on the model, Label Spreading penalizes misclassified labeled samples rather than banning it completely. For the existence of data noise, Label Prapogation has certain limitations and does not performs well. An labels in Label Propagation algorithm can only flow to unlabeled nodes, which may block some paths that need to be propagated through labeled nodes, which limits the propagation of information in the graph. Label Spreading algorithm enables labels to be broadcast to all adjacent nodes to improve this problem.  There are two optimization goals for Label Spreading. The first is the same as that of Label Propagation, but there is no restriction that the model's prediction results for labeled samples must be equal to its true label. The second is the prediction loss for labeled data with a penalty parameter as its weight. Due to different optimization goals, Label Propagation has a closed-form solution, while Label Spreading needs to be solved iteratively. In each iteration, a trade-off parameter is required to weight spreading results and initial labels of samples as the current prediction results.


#### Wrapper Method

不同于其他半监督学习方法用无标注数据与有标注数据共同学习一个学习器，封装方法基于一个或多个封装好的监督学习器，这些监督学习器往往只能处理有标注数据，因此封装类方法总是与无标注数据的伪标注密切相关。另外不同于其他方法需要固定学习器的形式，封装方法可以任意选择其监督学习器，具有非常强的灵活性，便于将已有监督学习方法扩展到半监督学习任务，具有较强的实用价值与较低的应用门槛。

Unlike other semi-supervised learning methods that use unlabeled data and labeled data to train a learner together, Wrapper methods is based on one or more wrapper supervised learners. These supervised learners can often only process labeled data, so they are always closely related to pseudo-labels of unlabeled data. In addition, unlike other methods that require a fixed learner, wrapper methods can arbitrarily choose its supervised learner, which is very flexible and facilitates the extension of existing supervised learning methods to semi-supervised learning tasks. It has strong practical value and Low application threshold.

##### Self-Training

Yarowsky等[8]提出了Self-Training方法。Self-Training方法是最经典封装方法，该方法是一种迭代方法，首先利用有标注数据训练一个监督分类器，然后再每一轮迭代中，用当前学习器对无标注数据进行预测，得到其伪标注，之后取自信度高于一定阈值的无标注样本及其伪标注与有标注数据结合，形成新的混合数据集，在混合数据集上训练新的分类器，用于在下一轮迭代过程中预测无标注数据的伪标注。在训练方法简单便捷，且可以使用任意可以提供软标注的监督学习器，为后续其他方法的研究提供了基础。

Self-Training was proposed by Yarowsky et al. The Self-Training is the most classical wrapper method. It is an iterative method. Firstly, a supervised classifier is trained with labeled data. Then in each round of iteration, the current learner is used to make prediction on unlabeled samples to obtain its pseudo labels. Unlabeled samples with their pseudo labels whose confidence higher than a certain threshold are combined  with labeled dataset to form a new mixed dataset. Lately a new classifier trained on the mixed data set is used in the next iteration process. The training pocess of Self-Traing is so convenient that any supervised learner which provides soft labels can be used. Self-Training provides a basis for subsequent research on other wrapper methods.


##### Co-Training

Blum等[10]提出了Co-Training。Co-Training即协同训练方法，用两个基学习器互相协同，辅助彼此的训练，即对于在一个学习器上自信度比较高的无标注样本，Co-Training会将该样本与其伪标注传递给另一个学习器，通过这种交互的形式，一个学习器上学到的知识被传递到了另一个学习器上。由于两个基学习器存在差异，其差异性决定了它们对于相同样本的学习难度会不同，Co-Training有效地利用了这种不同，使单独地学习器不仅可以使用其本身自信的伪标注，还可以使用另一个学习器自信的伪标注，加大了对无标注数据的利用程度，最后将两个学习器进行集成作为最终的预测结果。为了使两个基学习器有一定的差异，Co-Training采用了多视图假设，即基于相同数据的不同样本特征集合训练的模型应该对相同样本有相同的预测结果。Co-Training将样本的特征划分为两个集合，作为对样本从两个不同视图的观测，初始状态两个学习器的训练数据仅为不同视图下的有标注数据，在迭代过程中，在一个学习器上伪标注自信度高的无标注样本及其伪标注会被同时加入两个学习器的训练数据集，用于下一轮的训练，迭代持续至两个学习器的预测均不再发生变化为止。

Co-Training was proposed by Blum et al. In Co-Training, two basic learners are used to cooperate with each other to assist each other's training. For unlabeled samples with high confidence on one learner, Co-Training will pass them and their pseudo-labels to another learner, through this form of interaction, the knowledge learned on one learner is passed on to another learner. Due to the difference between the two base learners, their learning difficulty for same samples is different. Co-Training effectively takes advantage of this difference, so that a learner can not only use its own confident pseudo-labels, but also use another learner's confident pseudo-labels to increase the utilization of unlabeled data. Finally the two learners are ensembled to be used for prediction. In order to make the two base learners have a certain difference, Co-Training adopts the multi-view assumption, that is, models trained based on different feature sets should have the same prediction results for the same samples. Co-Training divides the features of the samples into two sets, which are used as observations of the samples from two different views. The training data of the two learners in the initial state are only labeled data in different views. In the initial state, the training datasets of the two learners are only labeled dataset in different views. In the iterative process, Unlabeled samples with high pseudo-label confidence on one learner are added to the training datasets of two learners at the same time, which will be used for the next round of training. The iteration continues until the predictions of both learners no longer change.

##### Tri-Training

Zhou等[11]提出了Tri-Training方法。由于Co-Training等多学习器协同训练的方法必须要求基学习器之间需要存在差异，如数据视图不同或模型不同。但在实际应用中，可能只有单一视图的数据，且人为对原始数据进行特征切割会一定程度上损失特征间关系的相关信息，且对于划分方式需要一定的专家知识，错误划分可能会导致严重的性能下降；而采用不同类型的模型进行协同训练又需要设置多个监督学习器，考虑到封装方法较其他半监督学习方法的优势在于可以直接将监督学习算法扩展到半监督学习任务，因此实际应用中使用封装方法的场景往往只有一个监督学习算法，设计额外的监督学习算法一定程度上损失了封装方法的便捷性。Tri-Training方法从数据采样角度解决了这一问题，仅使用一个监督学习算法，对数据进行多次有放回随机采样（Boostrap Sample），生成多个不同的训练数据集，从而达到用同一算法学习得到多个不同模型的目的。不同于其他封装方法采用模型对无标注样本的自信度作为是否将无标注数据纳入训练数据集的依据，但是有些情况下模型会出现对错误分类过度自信的情况，导致伪标注与真实标注间可能存在较大的偏差，Tri-Training使用三个基学习器协同训练，在一个基学习器的训练过程中，对于无标注样本可以用另外两个基学习器判断是否应该将其纳入训练数据，如果在一轮迭代中，两个学习器的共同预测错误率较低且对于该无标注样本拥有相同的预测结果，那么这一无标注数据及其伪标注更可能会对当前训练的基学习器产生积极影响，使用两个模型的预测一致性判断是否使用无标注样本的方法相较仅使用一个模型的自信度的方法更加具备稳健性。另外不同于其他封装方法被选中的无标注数据会已知存在于训练数据中，这可能会导致被错误预测的无标注样本会对学习器造成持续性影响，永远无法被纠正，在Tri-Training算法中每一轮迭代采用的无标注数据集及其伪标注都会被重新选择。Tri-Training拥有扎实的理论基础，并基于理论基础在每轮迭代中对无标注数据的利用增加了限制条件，这些限制条件主要针对一个基学习器每一轮迭代训练中另外两个学习器的共同预测错误率和无标注数据的使用数量：当另外两个基学习器的共同预测错误率较高时，即使它们的预测结果具有一致性，都应该放弃在训练过程中使用无标注数据；当满足一致性的样本数量过多时，也应避免过度使用无标注数据，而是根据理论结果得到该轮迭代中使用无标注数据的数量上界，如果超出上界则应该通过采样进行缩减。对无标注数据使用的严格约束极大程度上增加了半监督模型的安全性，可以有效缓解因错误引入无标注数据导致模型性能下降的问题。

Tri-Training is proposed by Zhou et al. Because multi-learner training methods such as Co-Training must require differences between basic learners, such as different data views or different models. However, in practical applications, there may only be a single view of data, and artificially cutting the original data will lose the information of the relationship between the features in different views. The division method for views requires expert knowledge because wrong division may lead to serious performance degradation. The use of different models for co-training requires setting up multiple supervised learners. Considering that the advantage of wrapper methods over other semi-supervised learning methods is that the supervised learning algorithm can be directly extended to semi-supervised learning tasks, so there is often only one supervised learning algorithm in the scenarios where the wrapper methods are used. Designing additional supervised learning algorithms loses the convenience of wrapper methods. Tri-Training solves this problem from the perspective of data sampling. Only one supervised learning algorithm is used, and the dataset is randomly sampled multiple times to generate multiple different training datasets to achieve the purpose of learning to get multiple different models. In other wrapper methods, the model's confidence in unlabeled samples is used to decide whether to incorporate unlabeled data into the training data set. However, in some cases, the model may be overconfident in the misclassification, resulting in a large deviation. Tri-Training uses three base learners for training. During the training process of one base learner, for unlabeled samples, the other two base learners can be used to judge whether they should be added in the training dataset. If in one iteration, the common prediction error rate of the two learners is low and they have the same prediction result for the unlabeled sample, the unlabeled sample and its pseudo-label are more likely to produce positive effect to the current training base learner. Using the prediction consistency of the two models to determine whether to use unlabeled samples is more robust than using the confidence of only one model. In addition, in other wrapper methods, the selected unlabeled samples will always exist in the training dataset, which may cause the unlabeled samples that are mispredicted to have a lasting impact on the learner and can never be corrected. In Tri-Training, the unlabeled samples and their pseudo-labels used in each iteration of the algorithm are reselected. Tri-Training has a solid theoretical foundation and adds restrictions on the use of unlabeled data in each iteration based on the theoretical foundation. Strict constraints on the use of unlabeled data greatly increase the security of the semi-supervised model, which can effectively alleviate the problem of model performance degradation caused by erroneously using unlabeled data.

#### Ensemble Method

在机器学习领域，使用单个学习器容易因欠拟合或过拟合造成模型偏差或方差过高，使模型泛化能力不足，集成学习将多个弱学习器结合起来，既提高了模型对假设空间的表示能力，又减弱了因单一学习器的错误造成的影响，提高了模型的可靠性。在半监督学习领域，由于无标注数据的加入，使用单一学习器为无标注数据设置伪标注这一做法使单一学习器的不稳定性进一步加剧，对有效的集成学习方法有更强的依赖。

In the field of machine learning, the use of a single learner is likely to cause high deviation or variance due to underfitting or overfitting, resulting in insufficient model generalization ability. Ensemble learning combines multiple weak learners, which not only improves the model's ability to represent the hypothesis space, but also reduces the impact of errors caused by a single learner and improves the reliability of the model. In the field of semi-supervised learning, due to the addition of unlabeled data, using a single learner to set pseudo-labels for unlabeled samples further exacerbates the instability of a single learner and has a stronger reliance on effective ensemble learning methods.

##### Assemble

Bennett等[12]提出了Assemble方法。Assemble即自适应监督集成，是基于自适应提升（AdaBoost）方法在半监督学习领域的扩展。提升（Boosting）方法是集成学习中的一类重要方法，这类方法通过当前集成学习器的预测效果对数据集进行采样，采样过程会更加重视目前集成学习器预测效果不佳的样本，用采样后的数据训练新一轮的学习器，这一策略使模型在每一轮新的弱学习器学习过程中可以更多关注目前集成学习器学习效果较差的样本，不断提高模型的泛化能力和稳健性。AdaBoost是Boosting类方法中最具代表性的方法，该方法根据模型预测结果与样本自身标注的差异自适应地调整样本权重，并根据样本权重对数据集进行采样用于学习下一轮迭代弱学习器的学习，并根据每一轮弱学习器的准确率确定其权重，加入到集成学习器中，其中准确率更高的弱学习器拥有更高的集成权重。ASSEMBLE通过对无标注数据添加伪标注的方法将AdaBoost方法在半监督学习领域进行了推广，初始阶段无标注样本的伪标注为与其最接近的有标注样本的标注，且无标注数据与有标注数据拥有不同的权重，在迭代过程中，每一轮无标注数据的伪标注被更新为该轮集成学习器的预测结果，随着迭代的进行，集成学习器效果越来越好，伪标注也越来越准确，进一步推动着新一轮弱学习器对集成学习器产生更有益的影响。

Assemble is proposed by Bennett et al. Assemble is an extension of AdaBoost method in the field of semi-supervised learning. The Boosting method is an important method in ensemble learning. This method samples the data set through the prediction effect of the current ensemble learner. The sampling process will pay more attention to the samples whose results of the current ensemble learner is not good. This strategy enables the model to pay more attention to the samples with poor learning effect of the current ensmeble learner in each round of new weak learner learning process, and continuously improve the generalization ability and robustness of the model. AdaBoost is the most representative method in the Boosting methods. This method adaptively adjusts the sample weight according to the difference between the prediction results and the samples' real labels. The weak learners with higher accuracy have higher ensemble weights. ASSEMBLE promotes the AdaBoost method in the field of semi-supervised learning by adding pseudo-labels to unlabeled data. In the initial stage, the pseudo-labels of unlabeled samples are the labels of the closest labeled samples and the unlabeled data and labeled data have different weights. In the iterative process, the pseudo-labels of unlabeled data are updated to the prediction results of the ensemble learner in each round. As the iteration progresses, the effect of the ensemble learner is getting better and better and the pseudo-labels are more and more accurate, which further have a more beneficial impact on the ensemble learner.

##### SemiBoost

Mallapragada等[13]提出了SemiBoost方法（如图2-3所示）。不同于Assemble方法仅将模型的预测结果与真实标注或伪标注之间的差异性作为对样本采样的依据，没有考虑样本之间的关系，SemiBoost基于图半监督学习方法，指出在采样时应该将样本间的相似度也纳入考量，应该对样本间相似度较高但目前集成学习器的预测结果不一致性较大的样本设置更大的权重，SemiBoost使集成学习器在不断迭代过程中，不仅提高了模型的泛化能力，还提高了模型对于相似样本预测的一致性，使模型更加稳健，这一过程中无标注样本发挥了更大的作用。SemiBooost每一轮迭代中对于新的弱学习器进行学习，其优化目标由两项组成，第一项以有标注样本集与无标注样本集间的相似度作为权重，惩罚了无标注数据的伪标注和与其相近的有标注数据的真实标注之间的差异性，这接近于标注传播的效果，使得模型可以根据图结构通过样本相似性和有标注数据的真实标注得到无标注数据的伪标注，使伪标注更大程度上接近真实标注；第二项以无标注样本集内部的相似度作为权重，对相似度较高的样本之间预测差异较大的无标注样本赋予更高的权重，缓解了噪声对模型的影响。

SemiBoost are proposed by Mallapragada et al. Unlike Assemble, which only uses the difference between the prediction results of the model and the real labels or pseudo-labels to weight samples and does not consider the relationship between samples, SemiBoost is based on graph semi-supervised learning method, which points out that the similarity between samples also should be taken into consideration and a larger weight should be set for the samples with high similarity in feature space and high inconsistency in prediction results to other samples. The generalization ability and robustness of model are improved. Unlabeled samples play a greater role in this process. SemiBooost learns a new weak learner in each round of iteration. Its optimization objective consists of two items. The first item punishes the discrepancy between pseudo-labels of unlabeled samples and real labels of labeled samples which uses the similarity in feature space as weights. It is close to the effect of Label Propagation so that the model can obtain pseudo-labels of unlabeled samples according to the graph structure. The second term penalizes the prediction between unlabeled samples which uses the similarity within the unlabeled samples as weights. Te second item alleviates the impact of noise to the model.

#### Semi-supervised Regression

目前大多数半监督学习算法都是针对分类任务而设计的，且不能自然地扩展到回归任务，仅有少部分工作针对半监督学习回归任务，这很大程度上是因为回归任务相较分类任务更难提出合理的假设，研究半监督回归相较半监督分类有着更多的困难。目前这一领域还有待更多的研究成果，半监督回归任务在现实场景中依然具备较大的需求和应用价值。

Most of the current semi-supervised learning algorithms are designed for classification tasks and cannot be naturally extended to regression tasks. Only a few works are aimed at semi-supervised learning regression tasks. It is because in regression tasks, reasonable assumption are more difficult to be made compared to classification tasks. At present, there are few research results in this field and the semi-supervised regression task still has great demand and application value in real scenarios.

##### CoReg

Zhou等[14]提出了CoReg方法。CoReg将Co-Training算法引入了回归任务，在原本用于分类的封装类算法中，往往假设模型越自信的样本越会对之后的训练产生积极影响，因此将模型预测结果中类别概率的最大值作为模型对该样本的自信度，根据学习器对无标注数据预测的自信度选择一部分无标注样本及其伪标注加入到训练数据中参与之后迭代的训练，但是在回归任务中难以评估模型对无标注样本的自信度，因此难以选择无标注样本加入训练过程，这也是这一类方法难以应用于回归任务的一个重要原因。CoReg解决了这一问题，从而将Co-Training算法应用在了回归任务中。CoReg使用k近邻（kNN）模型作为基学习器，对于两个基学习器，为了保持它们之间存在差异性，分别使用了不同的阶数计算闵可夫斯基（Minkowsky）距离作为k近邻模型中样本间的距离。为了度量模型对样本的自信度，对于每一无标注样本，模型先预测其实值伪标注，将其与所有参与训练的样本结合起来重新训练一个学习器，并用均方误差损失评估该样本对它的k个近邻结点产生的影响，如果这些k近邻结点的均方误差降低，说明加入该无标注样本更可能对之后的训练产生更积极的影响，因此CoReg将加入一个无标注样本前后均方误差的差异作为自信度的评估标准，每一轮迭代将一个学习器上自信度最高的无标注样本及其实值伪标注加入到另一个学习器的训练数据中，从而完成了Co-Training的训练过程。

CoReg is proposed by Zhou et al. CoReg introduces the Co-Training algorithm into regression tasks. In wrapper methods originally used for classification, it is often assumed that samples with higher confidence will have a more positive impact on subsequent training. Some unlabeled samples and their pseudo-labels are selected as training data according to the confidence. However it is difficult to get the confidence in the regression task. CoReg solves this problem, thereby applying the Co-Training algorithm to regression tasks. CoReg uses the kNN model as the type of base learner. For two base learners, in order to maintain the difference between them, different orders are used to calculate the Minkowsky distance between samples in the k-nearest neighbor model. In order to measure the confidence of the model on the samples, for each unlabeled sample, the model first predicts the real-valued pseudo-label. CoReg then combines its pseudo-label with all the samples participating in the training process to retrain a learner and uses the mean squared error loss to evaluate to evaluate the impact of the sample on its k nearest neighbors. If the mean square error of these k-nearest neighbor nodes decreases, it means that adding the unlabeled sample is more likely to have a positive impact on subsequent training. Therefore, CoReg uses the difference between the mean square error before and after adding the unlabeled sample as the evaluation standard of confidence. The unlabeled sample with the highest confidence and its real value pseudo-label are added to the training dataset of another learner, thus completing the training process of Co-Training.

#### Semi-supervised Cluster

不同于半监督分类和半监督回归任务，用无标注数据辅助监督学习的过程，半监督聚类任务在原本无监督聚类的基础上引入了监督信息以辅助无监督学习的过程，其中监督信息不一定是有标注数据，也可能是其他与真实标注有关的知识，由于监督信息的不同也产生了不同多种半监督聚类方法。

Unlike semi-supervised classification and semi-supervised regression tasks, which use unlabeled data to assist the process of supervised learning, semi-supervised clustering tasks introduce supervision information to assist the process of unsupervised learning. Supervision information is not necessarily labeled data, but may also be other knowledge related to real labels. Due to the difference of supervised information, various semi-supervised clustering methods have also been proposed.

##### Constrained k-means

Wagstaff等[15]提出了Constrained k-means算法。该算法在k-means聚类算法的基础上引入了称为必连（Must Link）和勿连（Connot Link）的约束作为监督信息，其中必连约束限制了一些样本必须属于同一聚类簇，而勿连约束限制了一些样本必须属于不同的聚类簇，且必连约束与勿连约束存在传递机制，如A与B必连且B与C必连则A与C必连，A与B勿连且B与C必连则A与C勿连。k-means算法将样本归属于簇中心与样本最近的簇，与之相似的是Constrained k-means算法也会优先考虑簇中心与样本最近的簇，但与之不同的是Constrained k-means算法会在将一样本归为一个簇时，会首先判断该样本与簇内样本间是否违反必连和勿连约束，如果违反，Constrained k-means会重新考虑下一个符合条件的簇，如果所有簇都不能满足约束，则会发出聚类失败的警告，需要随机选择不同的聚类中心重新初始化。

Constrained k-means was proposed by Wagstaff et al. Based on k-means clustering algorithm, the algorithm introduces constraints called Must Link and Connot Link as supervision information. The Must Link constraint restricts that some samples must belong to the same cluster. The Connot Link constraint restricts that some samples must belong to different clusters. There is transfer mechanisms in constraints. For example, if A and B must be linked and B and C must be linked, then A and C must be linked and if A and B must be linked and B and C can not be linked, then A and C can not linked. K-means algorithm assigns a sample to the cluster whose cluster center is closest to the sample. Similarly, Constrained k-means also give priority to the cluster whose center is closest to the sample, but the difference is that Constrained k-means algorithm should judge whether the Must Link and Cannot Link constraints are violated between the sample and the samples which are already in the cluster. If violated, Constrained k-means will reconsider the next eligible cluster. If all clusters fail to satisfy the constraints, a warning of clustering failure is issued and different cluster centers are needed to be selected randomly to reinitialize the process.

##### Constrained Seed k-means

Basu等[16]提出了Constrained Seed k-means算法。该算法不同于Constrained k-means将必连和勿连约束作为监督信息，而是直接采用了有标注数据作为监督信息。由于有了部分有标注数据，可以通过直接在有标注数据集上计算类别均值的方式计算聚类中心，这有效缓解了聚类算法中因初始聚类中心选择的随机性造成的聚类不稳定，且可以将有标注数据集上的类别数量作为聚类算法中的簇数k，不需要再人为选择k值，避免了聚类时不合理的簇数选择造成的聚类结果不理想。不同于k-means算法在迭代过程中对所有的样本根据其余目前所有簇中心的距离判断其应归属的簇，Constrained Seed k-means算法在迭代过程中仅对无标注数据所属的簇进行更新，对于有标注数据会根据其真实标注固定其所属的簇，不会因簇中心的变化而改变。使用有标注数据参于聚类过成时聚类器更加可靠，缓解了无监督聚类的盲目性，有效地减弱了聚类结果与样本真实标注间差距过大和由于随机性带来的不稳定现象。

Constrained Seed k-means was proposed by Basu et al. This algorithm is different from Constrained k-means, which uses Must Link and Connect Link constraints as supervision information, but directly uses labeled data as supervision information. Since there are some labeled data, the cluster center can be calculated directly on the labeled dataset, which effectively alleviates the cluster instability caused by the randomness of the initial cluster centers selection. The number of classes of the labeled dataset can be used as the number of clusters in the clustering algorithm, which avoids the bad clustering results caused by unreasonable k value selection. Unlike k-means algorithm, in which all samples are judged to which cluster they should belong in the iterative process, Constrained Seed k-means algorithm only updates the cluster labels of unlabeled data. For labeled samples, their cluster labels are fixed with their real labels and not change as the change of cluster centers. The clusterer is more reliable when using labeled data to participate in the clustering process, which alleviates the blindness of unsupervised clustering and effectively reduces the large gap between the clustering results and the real labels of the samples. It also alleviates the instability caused by randomness.

### Deep Semi-supervised Learning

#### Consistency Regularization

深度学习方法通过设置损失函数，以梯度下降的优化方法引导模型训练的方向。一致性正则方法往往基于一致性假设，即假设对于样本增加一定程度的扰动，其预测结果应尽可能保持一致，从而在损失函数中引入了关于一致性的正则化项，使没有标注的样本也能参与到模型训练的过程中来，有助于提升模型对于噪声的稳健性。

Deep learning methods guide the direction of model training by setting the loss function with gradient descent. Consistency regularization methods are based on the assumption of consistency, which assumes if a certain degree of disturbance is added to samples, the prediction results should be consistent with the previous. These methods often introduces a consistency regularization term into the loss function which enables unlabeled samples to participate in the model training process to improve the robustness of the model to noise

##### Ladder Network

Rasmus等[17]提出了LadderNetwork方法（如图2-4所示）。该方法采用了自编码器结构，其中编码器最后一层的输出为分类软标注，即编码器同时具有分类功能，并采用两种编码方式，第一种为不带噪的编码器结构，即经典的编码器结构，第二种为带噪的编码器结构，即在经典的编码器基础上每一层的输入都会加入一定的噪声。LadderNetwork方法首先对样本分别进行带噪编码与不带噪编码，得到每个层次的带噪编码表示和不带噪编码表示；之后用解码器对带噪编码结果进行解码，得到每个层次的带噪解码表示；最后用均方误差损失（MSE）计算每一层次（包括原始输入数据作为第零层）的不带噪编码表示与带噪解码表示的不一致性，并通过原先确定的权重对各层次的不一致性进行加权作为无监督损失函数，从而利用无标注数据提升模型预测的稳健性。LadderNetwork算法的一致性正则化将带噪编码表示作为桥梁，惩罚了不带噪编码表示与带噪解码表示间的不一致性，一方面可以得到一个自编码器，使模型编码器与解码器各层次的表示可以保持一致，解码器利用编码后的结果可以尽可能地还原编码器各层表示以及原始数据；另一方面也可以使模型在存在噪声的情况下，保证隐层表示与没有噪声时尽可能一致，可以对抗微小的扰动。

LadderNetwork was proposed by Rasmus et al. This method adopts an autoencoder structure, in which the outputs of the last layer of the encoder are soft labels. The LadderNetwork adopts two encoding methods, the first is the classical encoder without noise, that is, and the second is the encoder with noise, which add noise to inputs of each layer of the classical encoder.   LadderNetwork firstly performs noise encoding and non-noise encoding on the samples respectively and obtains the noisy representation and the non-noisy representation of each layer. Then the decoder is used to decode the noisy encoding result, and the noisy decoding representations of each layer are obtained. Finally, mean square error(MSE) loss is used to calculate the inconsistency between the non-noisy encoded representation and the noisy decoded representation at each layer, including the original input data as the zeroth layer. The previously determined weights are used to determine the weights of inconsistency of each layer. Hierarchical inconsistencies are weighted as an unsupervised loss function, thereby improving the robustness of model. The consistency regularization of LadderNetwork uses the noisy encoded representation as a bridge to penalize the inconsistency between the non-noisy encoded representation and the noisy decoded representation. On the one hand, an auto-encoder can be obtained to make the representations of the encoder and the decoder consistent at all levels. On the other hand, the hidden layer representations keep consistent regardless weather noise is added, which makes the model can against disturbances.

##### UDA

Xie等[18]提出了UDA方法（如图2-5所示）。不同于LadderNetwork，UDA只对输入数据进行扰动，并不对隐层进行扰动，且UDA不一定采用高斯噪声进行扰动，而是可能可以采用多样的数据增广方式对数据进行增广。相比高斯噪声，UDA使用的数据增广，如图片旋转或文本替换等会对数据产生更大的影响，可以进一步提升模型的稳健性。UDA对无标注数据进行一次数据增广，之后比较增广前后的数据的预测结果，利用均方误差损失计算一致性正则项作为无监督损失，从而使无标注数据参与训练过程。

UDA was proposed by Xie et al. Unlike LadderNetwork, UDA only perturbs the input samples instead of all inputs of hidden layers. And UDA does not necessarily use Gaussian noise for perturbation, but may use various data augmentation methods. Compared with Gaussian noise, data augmentation used by UDA, such as image rotation or text replacement have a greater impact on the data, which can further improve the robustness of the model. UDA performs data augmentation on the unlabeled samples and then compares the prediction results before and after the augmentation. The mean square error loss is used to calculate the consistency regularization term as the unsupervised loss.

##### Pi Model

Laine等[19]提出了Pi Model方法（如图2-6所示）。不同于UDA将无标注数据进行一次增广后比较增广前后的数据的预测结果，计算一致性正则项，Pi Model分别对数据进行两次随机数据增广，并分别将两次增广的结果作为神经网络模型的输入进行预测，将预测结果的不一致性作为无监督损失，从而将无标注数据引入训练过程。由于增广过程的随机性，该方法两次增广会得到两项语义相似但特征可能略有不同的数据，通过一致性正则使模型对拥有一定界限的不同增广结果能产生相近的预测结果。

Pi Model was proposed by Laine et al. Unlike UDA, which augments the unlabeled data once and compares the prediction results before and after the augmentation and calculates the consistency regular term. Pi Model augments the data twice randomly and respectively uses the results of the two augmentations as inputs of the neural network model to get prediction results. The inconsistency of the prediction results are used as the unsupervised loss. Due to the randomness of the augmentation process, the two augmentations of this method will obtain two pieces of samples that are semantically similar but may have slightly difference in features. Through the consistency regularization, the model can produce similar prediction results for different augmentations with a certain range. 

##### Temporal Ensembling

Laine等[20]还提出了Temporal Ensembling方法（如图2-7所示）。该方法对Pi Model进行了一些改进。在Pi Model中，对于每个无标注数据，Pi Model需要分别对其进行两次增广和两次伪标注预测以计算其结果的不一致性，这带来了较大的算力消耗。Temporal Ensembling方法将其中一次伪标注预测改为对历史伪标注的指数移动平滑（EMA），即对同一数据的历史预测结果进行加权平均从而将历史预测集成起来，其中每一轮的伪标注权重会随着后续轮次的增加以指数级别的速度衰减。这种集成方式在有效地保留了历史伪标注信息，通过计算当前伪标注与历史伪标注间的一致性作为得到函数，并在每一轮次结束时更新历史伪标注。EMA方法极大程度上保障了模型的稳健性，避免了模型过度受单轮预测的影响，也减慢了模型对历史信息的遗忘速度，且每一轮次中对于每个数据只需要进行一次增广和预测，历史信息仅需进行一次加权平均即可维护，相较Pi Model极大地减少了算力消耗。

Temporal Ensembling are proposed by Laine et al. This method makes some improvements to Pi Model. In Pi Model, for each unlabeled sample, Pi Model needs to perform two augmentations and calculate the inconsistency of their prediction results, which brings a large consumption of computing power. Temporal Ensembling method changes one of the pseudo-label predictions to exponential moving average(EMA) of historical pseudo-labels, which is a weighted average of historical results. The weights of pseudo-labels decay exponentially in each round. This ensemble method effectively preserves the historical pseudo-labels information and get unsupervised loss by calculating the consistency between the current pseudo-label and the ensemble of historical pseudo-labels. Tthe historical ensemble is updated at the end of each epoch. EMA guarantees the robustness of the model. It avoids the model being overly affected by a single round of prediction and slows down the model’s forgetting speed of historical information. Temporal Ensembling only needs to augment and predict once for each sample in each round. Historical information can be maintained with only one weighted average calculation, which greatly reduces computing power consumption compared to Pi Model.

##### Mean Teacher

Tarvainen等[21]提出了Mean Teacher方法（如图2-7所示）。该方法借助了知识蒸馏的思想，即将教师模型的预测结果作为伪标注，用于训练学生模型，确保教师模型与学生模型预测结果的一致性，从而将知识由较为复杂的教师模型蒸馏到较为简单的学生模型。经典的知识蒸馏方法的目的在于模型的简化，即教师模型采用较为复杂的模型，学生模型采用较为简单的模型，而Mean Teacher的目的在于通过一致性使无标注数据参与学习过程并提升模型的稳健性，因此教师模型并非是复杂模型，而是在学生模型的基础上对参数进行指数移动平滑，这相对于经典的知识蒸馏方法减少了计算开销。Temporal Ensembling方法对每一轮次的预测结果进行指数移动平滑的计算，但是只有在每一轮次结束时才会对整体的历史信息进行更新，对于大型的数据集，会导致历史信息不能及时对同一训练轮次（Epoch）后续批次（Batch）的数据产生影响，会导致对历史信息的利用不够及时的问题。不同于Temporal Ensembling，Mean Teacher改为对模型参数采用指数平滑计算，在每一批次训练结束后都会及时更新模型参数的历史信息，有效地解决了历史信息更新与利用不及时的问题，这使得Mean Teacher方法更加灵活，通用性更强。

Mean Teacher was proposed by Tarvainen et al. This method relies on the idea of ​​knowledge distillation, where the prediction results of the teacher model are used as pseudo-labels to train the student model to ensure the consistency of the prediction results of the teacher model and the student model, thereby distilling knowledge from a more complex model to a simpler one. The purpose of the classical knowledge distillation method is to simplify the model, but the purpose of Mean Teacher is to make unlabeled data participate in the learning process and improve the robustness of the model. Therefore, the teacher model is not a complex model, but performs exponential moving average on the parameters based on the student model, which reduces the computational cost compared to the classical knowledge distillation method. Temporal Ensembling method performs EMA on the prediction results of each round, but the overall historical information only can be updated at the end of each round. Especially for large data sets, the historical information cannot be updated in time. Different from Temporal Ensembling, Mean Teacher uses EMA for model parameters and updates the historical information of model parameters in time after each batch of training. Mean Teacher is more flexible and general because it effectively solves the problem of untimely update and utilization of historical information. 

##### VAT

Miyato等[22]提出了VAT。不同于对数据增加随机噪声的方法，VAT的思想在于对数据增加对抗噪声，使模型在数据受一定限制条件下噪声影响时的最坏表现可以更好，这对应了博弈问题中的零和博弈问题和优化问题中的最小最大化问题。对于经典的监督对抗算法，通常将真实标注与模型预测结果之间的交叉熵损失作为对抗优化的目标，首先通过内层优化得到对于当前模型和数据使损失最大的噪声，之后通过外层优化得到在对数据施加噪声的情况下的模型参数，内外优化交替进行，使模型在应对数据噪声时可以在最坏情况下表现得不会太差。其中，外层优化为对模型参数得优化，往往通过梯度下降来进行，而内部优化是针对数据噪声的优化，该优化不存在闭式解，且因针对不同数据应采用不同的对抗噪声，不适宜用梯度优化，需要对最优噪声进行近似，在经典的监督对抗算法中常采用线性近似，即先对无噪声数据进行预测并计算损失函数的值，进行梯度回传，得到对于无噪声数据的梯度，并将归一化后的梯度与噪声上界的乘积最为对抗噪声。
不同于经典的监督对抗算法，VAT需要解决半监督场景存在无标注数据的问题，即无法通过监督计算损失后回传梯度计算对抗噪声，为了解决这一问题，VAT算法采用了一致性策略，即将监督损失改为一致性损失，将利用真实标注计算损失改为利用模型分别对无噪声数据和噪声数据进行预测得到噪声伪标注与无噪声伪标注，计算二者间的一致性作为无监督损失。在VAT算法中，不同于监督的对抗算法，对于无标注数据内层优化无法采用线性近似，这是因为在监督的对抗算法的内层优化中，首先需要计算真实标注与模型对不加噪数据预测结果间的分类损失，而VAT用伪标注代替了真实标注，导致对于不加噪数据回传的梯度始终为0，无法得到梯度方向从而无法得到对抗噪声，因此VAT采用了二阶泰勒近似代替了线性近似，将计算对抗噪声的问题转化为了计算损失函数对于噪声的海森矩阵的主特征向量的问题。由于对于d维数据噪声，计算其海森（Hessian）矩阵的特征向量需要O\left(d^3\right)的时间复杂度，为了解决计算复杂度过高的问题，VAT采用了幂迭代（Power Iteration）方法求解近似的矩阵特征向量，即先对近似特征向量进行随机采样，并不断用矩阵与当前近似特征向量相乘得到新的近似特征向量，不断进行该过程即可在较低时间消耗的情况下得到较好的近似结果，为了进一步避免对海森矩阵的直接计算，VAT采用了有限差分（Finite Difference）方法近似求解矩阵与近似特征向量的乘积。相较其他基于一致性正则的方法，VAT方法采用对抗噪声比采用随机噪声可以进一步提升模型的稳健性，避免了随机性对实验结果的过度干扰，基于对抗的方法更加可靠地保证了模型在最坏情况下的表现，具有更好的理论基础，且VAT在计算对抗噪声时通过近似方法避免了过度的额外计算开销，并解决了监督对抗算法无法直接应用于无标注数据的困境。

VAT was proposed by Miyato et al. Different from the methods of adding random noise to the data, the idea of ​​VAT is to add adversarial noise to the data, so that the worst performance of the model can be better when the data is affected by noise within a certain range, which corresponds to the zero-sum game in game theory and Min-Max problem in optimization. For classical supervised adversarial algorithms, the cross-entropy loss between the real labels and prediction results is usually used as the goal of adversarial optimization. The noise that maximizes the loss for the current model and data is obtained through the inner layer optimization. The outer layer optimization is used to obtain the model parameters which minimizes the loss. Inner and outer optimization are alternately performed, so that the model can not perform too bad in the worst case when dealing with data noise. The outer optimization is to optimize the model parameters, which is often carried out by gradient descent, while the inner optimization is optimized for data noise, in which there is no closed-form solution. The inner optimization is not suitable to use gradient optimization and it is necessary to approximate the optimal noise. Linear approximation is often used in classical supervised adversarial algorithms. It firstly predicts on the clear data and calculate the value of the loss function. Then it carries out the gradient backward to obtain the gradient. Finally it takes the product of the normalized gradient and the noise upper bound as the adversarial noise. Different from classical supervised adversarial algorithms, VAT needs to solve the problem in semi-supervised scenarios where loss of unlabeled data cannot be calculated supervisely and then adversarial noise cannot be obtained by gradient. In order to solve this problem, VAT adopts the consistency strategy. It changes the supervised loss to the consistency loss which uses the model to predict on the clear data and the noisy data respectively to obtain the clear pseudo-labels and the noisy pseudo-labels. Then it calculates the consistency between them. In VAT, linear approximation cannot be used for the inner optimization on unlabeled data because it is necessary to calculate the classification loss with real labels and VAT replaces real labels with pseudo-labels resulting in the gradient returned is always 0. So VAT uses second-order Taylor approximation instead of linear approximation, so the problem of computing against noise is transformed into the problem of computing the principal eigenvectors of the Hessian matrix of the loss function for noise. When the noise of data is d-dimensional, the time complexity of calculating the eigenvector of Hessian matrix is O\left(d^3\right). In order to solve the problem of excessive computational complexity, VAT adopts power iteration method to solve the approximate matrix eigenvectors, which randomly samples the approximated eigenvectors and continuously multiply the matrix and the current approximated eigenvectors to obtain new ones. Continuously performing this process can consume less time. In order to further avoid the direct calculation of the Hessian matrix, VAT adopts the Finite Difference method to approximate the product of the matrix and the approximate eigenvector. Compared with other methods based on consistency regularity, the use of anti-noise in the VAT method can further improve the robustness of the model than random noise and avoid excessive interference of randomness on the experimental results because the performance in the worst case has a better theoretical basis. VAT avoids excessive additional computational overhead through approximation methods when calculating adversarial noise and solves the dilemma that supervised adversarial algorithms cannot be directly applied to unlabeled data.

#### Pseudo Labeling

基于伪标注的方法通过为无标注数据赋以伪标注从而使无标注数据对学习过程产生影响。且由于模型对于不同样本的自信度不同，基于伪标注的方法通常取自信度较高的样本与其伪标注参与训练过程。

Methods based on pseudo-labeling make unlabeled data affect the learning process by assigning pseudo-labels to unlabeled data. Since the confidence levels of the model for different samples are different, the method based on pseudo-labels usually takes samples with higher confidence and their pseudo-labels to participate in the training process.

##### Pseudo Label

Lee等[23]提出了Pseudo Label方法（如图2-10所示）。该方法为最基础的伪标注方法，其损失函数包括两项，分别是监督损失和无监督损失，两部分都是交叉熵损失函数。其中对于无标注数据，Pseudo Label方法对神经网络输出结果进行softmax运算，得到样本属于各类别的自信度，Pseudo Label取自信度最高的类别作为样本的伪标注，用伪标注计算交叉熵损失。另外，在每一轮次中，并不是所有无标注样本都会参与训练过程，Pseudo Label设置了一个阈值，只有当本轮无标注样本的伪标注自信度大于所设阈值时，才会参加训练过程。Pseudo Label还设置了超参数用于控制监督损失与无监督损失的比重，并采用了预热（warmup）机制，刚开始训练时，无监督损失比重较低，随着训练的进行，无监督损失的比重越来越大。

Pseudo Label was proposed by Lee et al. This method is the most basic pseudo-labeling method. Its loss function includes two items supervised loss and unsupervised loss, both of which are cross-entropy loss functions. For unlabeled data, the Pseudo Label performs softmax operation on the output of the neural network to obtain the confidence of classification. Pseudo Label takes the category with the highest confidence as the pseudo-label of the sample and uses the pseudo-label to calculate the cross-entropy loss. In addition, in each round, not all unlabeled samples will participate in the training process. They participate in the training process only when the confidence of them in this round are greater than the set threshold. Pseudo Label also sets hyperparameters to control the proportion of unsupervised loss and supervised loss and adopts a warmup mechanism. At the beginning of training, the proportion of unsupervised loss is low and as the training goes on, the proportion is increasing.

##### S4L

Beyer等[24]提出了S4L方法（如图2-11所示）。这一方法采用了自监督技术，其基本思想在于：无标注数据无法直接参与分类器的训练，但是可以利用自监督对表示层产生影响，使模型可以学到更好的隐层表示，从而有助于分类器的训练。该方法主要用于图像数据，随机取0^\circle、90○、180○、270○之一作为度数对图像进行旋转操作，将角度作为伪标注，旋转后的图像与角度即可形成数据对，可以训练一个对角度进行分类的神经网络模型，虽然神经网络最后的分类层与目标任务不同，但其学到的隐层表示有助于对真实任务的学习。对于有标注数据，S4L也会进行同样的处理，使其拥有两个标注，代表旋转度数的伪标注和用于目标任务的真实标注，S4L对有标注数据用了两个分类层，其中之一是与无标注数据共享的度数分类器，另一个是用于目标任务的真实类别分类器，两个分类层共用相同的隐层。通过上述方式，S4L在训练自监督分类器的同时，使模型可以学习到更好的表示，从而提升了模型对目标任务的分类效果。与预训练与微调范式不同，S4L不用提前训练模型，而是可以同时处理有标注数据和无标注数据，并且相互促进，且有标注数据也参与了自监督学习过程，对数据进行了更大程度的利用。S4L也可以推广到其他类型的数据，需要采用与之对应的自监督训练方法。

S4L was proposed by Beyer et al. This method uses self-supervised technology. The basic idea is that unlabeled data cannot directly participate in the training of classifiers, but self-supervision can be used to affect the representation layer, so that the model can learn better hidden layer representations. This method is mainly used for image data. One of 0^\circle, 90^\circle, 180^\circle, and 270^\circle is randomly selected as the degree to rotate the image, and the angle is used as a pseudo-label. A neural network model can be trained to classify angles. Although the final classification layer of the neural network is different from the target task, the learned hidden layer representation is helpful for learning the real task. For labeled data, S4L uses two labels, a pseudo label representing the degree of rotation and a real label for the target task. S4L uses two classification layers for labeled data, one of which is the degree classifier shared with unlabeled data, and the other is the true classifier for the target task, and both classification layers share the same hidden layer. Through the above methods, S4L enables the model to learn better representations while training the self-supervised classifier, thereby improving the classification effect of the model on the target task. Different from the pre-training and fine-tuning paradigms, S4L does not need to train the model in advance, but can process labeled data and unlabeled data at the same time to promote each other. Labeled data also participates in the self-supervised learning process of S4L. S4L can also be generalized to other types of data, and corresponding self-supervised training methods need to be adopted.

#### Hybird Method

不同的半监督学习技术之间往往不会冲突，很多常用的半监督学习算法不局限于仅使用一类技术，而是将一致性、伪标注等技术进行结合，各取所长，产生新的混合方法。混合方法可以同时利用不同技术的优势，从而达到更好的训练效果。由于同时使用了多种技术，混合方法往往更加具备通用性。

There are often no conflicts among different semi-supervised learning techniques. Many commonly used semi-supervised learning algorithms are not limited to using only one type of techniques, but combine techniques such as consistency and pseudo-annotation et al and use their own strengths to generate new hybrid methods. Hybrid methods can leverage the advantages of different techniques simultaneously to achieve better training results.

##### ICT

Verma等[25]提出了ICT方法（如图2-12所示）。ICT即插值一致性训练，通过Mixup[34]数据增广方法对数据与预测结果进行线性插值，通过比较模型对插值后样本的预测结果与模型对原始数据的预测结果的插值之间的一致性将无标注数据引入训练过程。Mixup由Beta分布生成一个混合参数，对两项数据按这一混合参数得到线性插值，得到两项数据的混合数据，以此实现数据增广。ICT方法的损失函数分为监督损失与无监督损失两部分，其中监督损失通过交叉熵函数计算，无监督损失则要通过插值一致性计算。对于每一批次的数据，ICT首先根据Beta分布采样一个混合参数，然后将该批次样本随机打乱，将打乱的批数据与未打乱的批数据以混合参数为比例进行Mixup混合，得到混合批数据，模型对未打乱批数据和混合批数据进行预测，得到未打乱预测结果与混合预测结果，并将未打乱预测结果按样本打乱顺序重新排列得到打乱预测结果，ICT将未打乱预测结果与打乱预测结果以和样本相同的混合参数进行线性插值，并将插值结果与混合预测结果间的不一致性作为无监督损失。对于混合后的无标注数据，ICT使模型输出的软标注接近于伪标注的混合，将一致性技术与伪标注技术结合起来，使模型更加稳健。

ICT was proposed by Verma et al[25]. The full name of  ICT is Interpolation Consistency Training. The data and prediction results are linearly interpolated by Mixup [34] which is a data augmentation method. ICT introduces unlabeled data into the training process by using the consistency between the model's predictions on the interpolated samples and the interpolation of the model's predictions on the original data. Mixup generates a parameter which means mixing ratio from the Beta distribution, and linearly interpolates two samples using the ratio parameter to obtain the mixed sample. The loss function of ICT is divided into two parts: supervised loss and unsupervised loss. The supervised loss is calculated by the cross entropy function and the unsupervised loss is calculated by the interpolation consistency. For each batch of data, ICT firstly samples a mixing parameter according to the Beta distribution. Then ICT randomly scrambles the batch of samples, and mixes the scrambled batch data with the unscrambled batch data in proportion to the mixing parameter. The model predicts on the unscrambled batch data and the mixed batch data to get the unscrambled prediction results and the mixed prediction results. Finally ICT linearly interpolates the unscrambled prediction results and the scrambled prediction results with the same mixing parameters as the samples and takes the inconsistency as the unsupervised loss. For mixed unlabeled data, ICT makes the soft labels output by the model close to the mix of pseudo-labels and combines consistency technology with pseudo-label technology to make the model more robust.

##### MixMatch

Berthelot等[26]提出了MixMatch方法（如图2-13所示）。该方法也用了Mixup方法，但不同于ICT仅对无标注数据的样本与伪标注进行Mixup，MixMatch对有标注数据与无标注数据进行了混合，并对混合后的数据样本及其标注与伪标注进行了Mixup。MixMatch首先对无标注数据多次增广并进行多次预测，通过对多次预测结果求均值并进行锐化得到无标注数据的伪标注，对数据进行多次不同增广使模型的伪标注更加具备可靠性，对伪标注进行锐化降低了标注分布的熵，使分类界限尽可能穿过样本的低密度区域；之后MixMatch对有标注数据与无标注数据进行了结合与打乱，使无标注数据集与有标注数据集形成了一个新的混合数据集，从混合数据集中取出与原有标注数据集相同数量的数据进行Mixup作为新的有标注数据集，将混合数据中剩余数据与无标注数据集进行Mixup得到新的无标注数据集；最后MixMatch分别对新有标注数据集和新无标注数据集进行预测，用新有标注数据集的预测结果计算交叉熵作为监督损失，用新无标注数据的预测结果计算均方误差作为无监督损失，通过权重参数将二者结合起来作为模型的损失函数。不同于其他方法将有标注数据与无标注数据分别计算损失，MixMatch将有标注数据与无标注进行了结合、打乱、重新划分，这降低了因错误的伪标注导致模型性能下降的风险。在原本仅使用伪标注训练的过程中加入真实标注，有助于利用真实标注辅助无标注数据的训练，引导无标注一致性的正确训练方向，既保障了一致性正则原有的稳健性，还能使模型不会因伪标注与真实标注不符过度偏离目标。

##### ReMixMatch

Berthelot等[27]还提出了ReMixMatch方法（如图2-14所示）。ReMixMatch是MixMatch的改进版本，其引入了两种技术：分布对齐和增广锚定。分布对齐目的在于使模型对于无标注数据预测得到的伪标注应与有标注数据的标注有相同的边缘概率分布，在深度学习中，模型的预测经常偏向数量较多的类别，另外MixMatch对软标注使用了锐化操作减少了标注分布的熵以促使分类边界尽可能通过低密度区域，这都导致了有标注数据的标注分布与无标注数据的伪标注分布产生了差异，这反映了为无标注数据赋予伪标注存在类别间的不公平现象，分布对齐技术有效缓解了这样的问题。分布对齐技术计算有标注数据的真实标注分布，在每一批次的训练中，计算其输出的软标注分布，对于一个样本的软标注，使其与真实标注分布与当前批次软标注分布的比值相乘得到对齐后的软标注，将对齐后的软标注进行锐化得到样本的伪标注。增广锚定是为了使模型适应更强的数据增广，对于监督学习方法，在一定程度内，对数据施加更强的数据增广可以进一步提升模型的泛化能力，但这是以监督学习中无论对样本施加强增广还是弱增广，标注都不会发生变化为前提。在半监督学习中，往往由模型对无标注数据的预测结果得到伪标注，伪标注会随着数据增广的形式而变化，如果对样本施加较强的增广，容易使伪标注过度偏离真实标注，无法发挥监督学习中强数据增广的作用，这也导致了MixMatch方法不能与较强的数据增广方式相容，ReMixMatch通过引入增广锚定技术首先对无标注样本进行弱数据增广，将模型对其预测的结果作为伪标注，并将其作为“锚”固定下来，这使得后续无论对无标注数据进行何种数据增广，都不会使其伪标注发生变化。ReMixMatch方法对无标注数据进行了一次弱数据增广和多次强数据增广，并都以模型对弱增广数据的预测结果经对齐与锐化后作为伪标注，由弱增广和所有强增广后的数据集组成更大的无标注数据集。之后ReMixMatch采用与MixMatch相同的策略对有标注数据集和无标注数据集进行组合、打乱与重新划分。另外，ReMixMatch的损失函数与MixMatch由较大的不同，ReMixMatch的有监督损失与无监督损失有采用交叉熵进行计算，且不同于MixMatch的损失函数仅包含监督损失与无监督损失两项，ReMixMatch增加了两项损失，这是由于MixMatch仅对Mixup后的数据集进行损失计算，虽然Mixup使模型拥有了更好的泛化性能，但是仅使用Mixup后的数据可能会忽略Mixup前数据集的一些信息，因此ReMixMatch从多个Mixup前的强增广数据集中取出一个，用于计算Mixup前数据的无监督损失作为损失函数第三项；ReMixMatch还借鉴了S4L的自监督策略，对取出的Mixup前的强增广数据集进行随机旋转并对其旋转角度进行预测，自监督进一步促进了模型隐层的学习，将对旋转角度分类的交叉熵损失作为自监督损失，用作损失函数的第四项。ReMixMatch以一个更为复杂的框架将多种技术融为一体，不仅结合了各方法的优势，且因为其全面性而更加通用。

##### FixMatch

Sohn等[28]提出了FixMatch方法（如图2-15所示）。FixMatch同样使用了强数据增广与弱数据增广，不同于ReMixMatch通过增广锚定技术利用弱数据增广固定无标注数据的伪标注，FixMatch更加关注模型对弱增广数据与强增广数据预测结果的一致性。与ReMixMatch相同的是FixMatch同样根据模型对弱增广数据的预测结果得到伪标注，FixMatch的伪标注为硬标注。之后FixMatch对无标注数据进行强增广，得到预测结果，FixMatch仅用模型自信的无标注数据进行训练，即设置一个阈值参数，仅当自信度大于阈值参数时，该数据才会参与训练过程。FixMatch利用模型对弱增广样本得到的伪标注和模型对强增广样本得到的预测结果计算交叉熵作为无监督损失，通过权重参数将无监督损失与监督损失结合起来作为FixMatch的损失函数。

FixMatch was proposed by Sohn et al. FixMatch also uses strong data augmentation and weak data augmentation. Unlike ReMixMatch, which uses augmented anchoring to fix pseudo-labels of unlabeled data by weak data augmentation. FixMatch pays more attention to the consistency of prediction results between weakly augmented data and strong augmented data. Similar to ReMixMatch, FixMatch also obtains hard pseudo-labels according to the prediction results of the model for weakly augmented data. After that, FixMatch augments the unlabeled data strongly to obtain the prediction results. FixMatch only uses the unlabeled data with which the model is confident for training using a threshold parameter. Only when the confidence is greater than the threshold parameter, the data will participate in the training process. FixMatch calculates the cross-entropy using the pseudo-labels obtained by the model for weakly augmented samples and the prediction results obtained by the model for strong augmented samples as unsupervised loss. Fixmatch combines the unsupervised loss and the supervised loss by a weight parameter as the final loss.

##### FlexMatch

Zhang等[29]提出了FlexMatch方法（如图2-16所示）。FlexMatch是对于FixMatch的改进，且注重于解决半监督学习中各类别间的不公平现象，FixMatch根据固定的阈值参数筛选自信度高的无标注样本及其伪标注参与模型训练过程，但有时虽然原始数据集是类别平衡的，但由于各类别学习难度不同，采用固定阈值进行筛选会导致一些难学习的类别相较易学习的类别更少参与训练过程，这样模型对较难学习的类别样本自信度更低，进一步加剧了参与训练的无标注数据的类别不平衡，这种不公平形成了恶性循环，造成了马太效应，导致模型对较难学习的类别学习效果越来越差，因此FlexMatch提出了对于不同的类别应采用不同的筛选标准，缓解因学习难度不同造成的类别不平衡现象。FlexMatch在FixMatch的基础上改用了动态阈值设置的方法，对较难学习的类别设置更低的自信度阈值，一种最基础的方法为设置一个验证集，根据模型在验证集上各类别的准确率设置阈值，但由于有标注的训练数据本身已较少且在训练过程中不断进行验证更新模型的验证准确率会造成较大的计算开销，因此FlexMatch采用了近似评估类别准确率的方法，首先选取自信度最高的类别作为其样本伪标注，对于每一批次的无标注数据，统计不同类别在该批数据中作为伪标注且自信度大于阈值参数的数量，之后对不同类别的统计数量除以其中的最大值进行归一化作为该类别的分类难度的评估度量，用固定阈值与该类别的分类难度度量相乘即可得到该类别在这一批次无标注数据中应使用的动态阈值。FlexMatch较好地缓解了无标注数据根据自信度进行筛选后由于学习难度不同造成的的类别不平衡问题，且没有因在训练过程中评估模型对不同类别的预测效果产生过多的额外计算时间和存储开销。

FlexMatch was proposed by Zhang et al[29]. FlexMatch is an improvement version of FixMatch and focuses on solving the unfair phenomenon between categories in semi-supervised learning. FixMatch selects unlabeled samples with high confidence and their pseudo-labels according to fixed threshold to participate in the training process. But sometimes although the original The dataset is class-balanced, due to the different learning difficulty of each class, using a fixed threshold for selecting will cause some classes which are difficult to learn to be less used in the training process than which are easy to learn. The model has lower confidence in the samples whose classes are more difficult to learn, which further exacerbates the class imbalance of the unlabeled data participating in the training. This unfairness forms a vicious circle and causes Matthew effect in Fixmatch. This unfairness forms a vicious circle, resulting in the Matthew effect, which causes the model to learn less and less well for the categories which are hard to learn. Therefore, different selecting criteria should be used for different categories to alleviate the class imbalance caused by different learning difficulties. FlexMatch uses dynamic threshold on the basis of FixMatch. It sets a lower confidence threshold for the classes that are more difficult to learn. One of the most basic methods is to set a validation dataset thresholds according to the accuracy rates on the validation dataset. However, since the labeled training data is relatively scarce, and the verification accuracy of the model is continuously updated during the training process, it will cause a large computational. Therefore, FlexMatch adopts the method of approximately evaluating the accuracy. Flexmatch firstly counts the number of times for each class that the class is consider as the pseudo-label and the confidence is greater than the threshold respectively for each batch of unlabeled data. After that, the statistics of different categories are divided by their maximum value and normalized as the evaluation of the classification difficulty. Finally, Flexmatch multiplys the fixed threshold by the classification difficulty metric of each category to get the dynamic thresholds for each category in current batch of unlabeled data. FlexMatch better alleviates the problem of class imbalance caused by different learning difficulties after unlabeled data is selected according to the confidence and does not cause excessive extra computing time and storage.

#### Deep Generative Model

生成式方法利用真实数据对数据分布进行建模，并且可以利用这一分布生成新的数据。不同于经典的生成模型，深度生成式模型基于深度神经网络生成数据。生成对抗网络（GAN）和变分自编码器（VAE）是最常用的生成式模型。

Generative methods use real data to model a data distribution, and this distribution can be used to generate new data. Unlike classical generative models, deep generative models generate data based on deep neural networks. Generative Adversarial Network(GAN) and Variational Autoencoder(VAE) are the most commonly used generative models.

##### ImprovedGAN

生成对抗网络模型分为两个部分：生成器和判别器，其中生成器假设数据可以由产生于某一特定的分布的低维隐变量生成，通过从隐变量分布上随机采样用于生成模拟数据，而生成器是一个分类器，用于判别输入样本是真实数据还是由生成器生成的模拟数据，生成器通过优化要使生成的样本与真实样尽可能接近以欺骗判别器，判别器通过优化要尽可能正确地区分真假样本，避免被生成器欺骗，两者以对抗地方式共同训练，从而达到同时得到较好的生成器与判别器的目的。

Generative Adversarial Network is divided into two parts: the generator and the discriminator, where the generator assumes that the data can be generated by low-dimensional latent variables generated from a specific distribution and is used to generate simulated data by randomly sampling from the latent variable distribution. The generator is a classifier, which is used to distinguish whether the input sample is real data or simulated data generated by the generator. The generator is optimized to make the generated samples as close as possible to the real samples to deceive the discriminator and the discriminator is optimized to distinguish real data or simulated data as accurately as possible to avoid being deceived by the generator. The two are trained together in an adversarial manner, so as to achieve the purpose of obtaining a better generator and discriminator at the same time.

Salimans等提出了ImprovedGAN （如图2-17所示）。经典的GAN模型仅利用无标注数据就可以完成训练，其判别器仅需要判断样本是真实样本还是生成样本。ImprovedGAN加入了对有标注数据的利用，要求判别器不仅要区分样本的真实性，还要完成对样本的分类，即将判别器改为k+1类分类器，其中k是原数据集的类别数量，通过生成器与判别器的交替训练，既可以实现数据生成，又可以完成分类。

ImprovedGAN was proposed by Salimans et al. Classical GAN model can be trained only with unlabeled data, and its discriminator only needs to judge whether the sample is a real sample or a generated sample. ImprovedGAN adds the use of labeled data, requiring the discriminator not only to distinguish the authenticity of the samples, but also to complete the classification of the samples. So the discriminator is changed to a k+1 class classifier, where k is the number of classes in the original dataset. Both data generation and classification can be achieved through the alternate training of the generator and the discriminator,.

##### SSVAE

变分自编码器将深度自编码器融入生成模型，同样假设存在产生于某一特定分布的低维隐变量，将隐变量作为原特征的表示向量，并通过深度神经网络建立隐变量到原特征的映射，作为解码器；由于无法直接求得原始特征到隐变量的后验概率，也需要通过神经网络来近似，作为编码器，学习的目标为做大化原始样本的边缘概率，由于当近似后验分布与真实后验分布相等时， 边缘概率可以达到其上界，因此可以学到与真实后验分布近似的祭祀后验分布，作为编码器可以得到合理的样本表示。

Variational Autoencoder integrates the deep autoencoder into the generative model. It also assumes that there are low-dimensional latent variables generated from a specific distribution. The latent variable is used as the representation vector of the original feature, and establish the mapping of latent variables to the original features as the decoder through the deep neural network. Since the posterior probability of meta-features to latent variables cannot be directly obtained, it also needs to be approximated by a neural network. As an encoder, the learning goal is to maximize the marginal probability of the original sample. VAT can learn a distribution that approximates the true posterior distribution and using it as an encoder can get a reasonable sample representation because when the approximate posterior distribution is equal to the true posterior distribution, the marginal probability can reach its upper bound.

Kingma等提出了SSVAE。经典的VAE模型仅利用无标注数据就可以完成训练，其目标在于通过编码器完成对数据表示的学习，并通过解码器可以实现数据生成。SSVAE加入了对有标注样本的应用，将编码器分为了两个部分，第一部分对原始数据进行编码得到样本软标注的概率分布，第二部分将原始数据与软标注共同作为输入得到隐变量的概率分布。经典VAE模型的编码器仅对数据的表示进行学习，SSVAE的编码器首先可以用样本进行分类，之后可以结合样本信息与类别信息共同学习样本的表示。

SSVAE was proposed by Kingma et al. Classical VAE model can be trained only with unlabeled data, and its goal is to complete the learning of data representation through the encoder, and realize data generation through the decoder. SSVAE adds the application of labeled samples, and divides the encoder into two parts. The first part encodes the original data to obtain the probability distribution of the soft labels of the samples, and the second part uses the raw data and soft labels as input to obtain probability distributions of the hidden variables. The encoder of the classical VAE model only learns the representation of the data. The encoder of the SSVAE can firstly classify the samples and then can combine the sample information and the category information to learn the representation of the samples.

#### Deep Graph Based Method

对于原始数据是图数据的情况，由于实例之间并非独立关系，而是通过边相连，经典的深度学习方法无法有效利用图模型的结构信息，因此无法直接将其应用于图数据。然而，图数据在实际应用中非常常见，研究可以用于图数据的深度学习方法具有重要意义，目前图深度学习已经取得了一定的研究成果。在半监督领域也是如此，经典的半监督学习方法忽略了图的结构信息，因此直接用于图结构数据效果并不理想，现实中的图数据任务往往都是半监督的，即待预测结点与训练节点在一张图上，图中同时存在有标注数据与无标注数据。

When the raw data is a graph, since the instances are not independent but connected by edges, the classical deep learning method cannot effectively utilize the structural information of the graph model, so it cannot be directly applied to the graph data. However, graph data is very common in practical applications and it is of great significance to study deep learning methods that can be used for graph data. At present, graph deep learning has achieved certain research results. The classical semi-supervised learning methods ignores the structural information of the graph, so the effect of directly applying them to graph-structured data is not ideal. In reality, graph data tasks are often semi-supervised, because the nodes to be predicted and the training nodes are on the same graph. There are both labeled data and unlabeled data in the graph.

##### SDNE

Wang等[32]提出了SDNE （如图2-18所示）。SDNE是一种可以在图中结点没有特征表示，仅有图结构信息的情况下学习图中结点嵌入向量的半监督图深度学习方法。该方法采用了自编码器结构，取结点在邻接矩阵中对应的行作为结点的邻接向量，将结点的邻接向量作为结点的特征输入自编码器，通过编码器得到结点的嵌入表示，通过解码器还原邻接向量，对于整个图，相当于通过自编码器还原了邻接矩阵。SDNE的损失函数主要包含三项：第一项惩罚了自编码器输入与输出的不一致性，使邻自编码器的输入与输出尽可能一致，另外与经典自编码器不同的是，SDNE的输入是邻接向量，由于邻接矩阵的稀疏性，导致输入的特征中存在大量的零值，SDNE指出应该更加关注对于非零值的还原，因此赋予了零值与非零值不同的权重；第二项为拉普拉斯正则，根据图结构信息惩罚了相邻节点间隐层表示的不一致性，并将邻接矩阵作为权重，得到了拉普拉斯正则项；第三项为L2正则，惩罚了自编码器的参数复杂度，以此来避免过拟合。在SDNE方法中，损失函数的第一项更关注结点本身的特征，而第二项更关注相邻节点间的信息，即图的结构信息，有效解决了经典半监督学习算法无法有效利用图结构信息的问题。

SDNE was proposed by Wang et al. SDNE is a deep semi-supervised graph learning method that can learn the embedding vector of nodes in the graph when there is no feature representation for the nodes in the graph and only graph structure information. This method adopts an autoencoder structure, takes the corresponding row of the node in the adjacency matrix as the adjacency vector of the node, and inputs the adjacency vector of the node as the feature of the node into the self-encoder, and obtains the embedding of the node through the encoder. means that the adjacency vector is restored by the decoder, which is equivalent to restoring the adjacency matrix by the autoencoder for the entire graph. The loss function of SDNE mainly includes three items: the first item penalizes the inconsistency between the input and output of the autoencoder, so that the input and output of the adjacent autoencoder are as consistent as possible. In addition, unlike the classic autoencoder, the input of SDNE is an adjacency vector. Due to the sparseness of the adjacency matrix, there are a large number of zero values ​​in the input features. SDNE points out that more attention should be paid to the restoration of non-zero values, so zero and non-zero values ​​are given different weights; the second item For the Laplacian regularity, according to the graph structure information, the inconsistency of the hidden layer representation between adjacent nodes is punished, and the adjacency matrix is ​​used as the weight to obtain the Laplace regularity term; the third term is the L2 regularity, which penalizes the self-regularity. The parameter complexity of the encoder to avoid overfitting. In the SDNE method, the first term of the loss function pays more attention to the characteristics of the node itself, while the second term pays more attention to the information between adjacent nodes, that is, the structural information of the graph, which effectively solves the problem that the classical semi-supervised learning algorithm cannot effectively utilize the graph. Problems with structural information.

##### GCN

Kipf等[33]提出了GCN。与SDNE使用结点的邻接向量作为结点特征学习嵌入表示不同，GCN更适用于结点本身存在特征的情况，GCN可以同时利用结点本身的特征信息和图结构信息进行学习，显著地提升了模型的效果。在图深度学习中，图神经网络（GNN）[35]是最常用的一类方法，这类方法通常以存在结点特征的图作为输入，可以学习到结点的深层表示，并以此完成学习任务。经典的GNN方法分为两个步骤：第一个步骤为聚集（Aggregate），即通过图结构将图近邻结点的信息及逆行汇集；第二个步骤为更新（Update），即根据结点自身表示与近邻结点更新结点表示。不断重复这两个步骤，可以得到每个结点的深层表示，由于聚集操作存在传播效果，结点的深层表示中不仅涵盖了节点自身信息，还涵盖了图结构信息。经典的聚集操作为线性聚集，即将近邻节点表示的线性组合作为该节点的近邻表示，经典的更新操作为使用感知机模型，由结点自身表示与紧邻表示得到新的自身表示。经典的GNN模型存在一定的局限性，其对紧邻节点的表示进行线性组合的聚集方式使度较大的结点更大程度地影响了其他节点，而度较小的结点对整个训练过程的影响较小。GCN方法对每一结点将标准化后的近邻表示与自身表示直接相加，并将结果作感知器的输入，得到的结果作为新的自身表示，其中标准化过程将近邻结点与自身结点的表示分别除以一个标准化因子，其中近邻结点的标准化因子为自身结点的度与近邻结点的度的几何平均，自身结点的标准化因子为自身结点的度。GCN在图结构任务上有着优异的表现，并且其更新过程避免了对近邻结点线性组合权重的学习，拥有更少的参数与更高的效率。

# API

## Algorithm

## Base

## Dataloader

## Dataset

## Distributed

## Evaluation

## Loss

## Network

## Optimizer

## Sampler

## Scheduler

## Split

## Transform

## utils

# FAQ

# Reference